---
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, fig.align = "center")
```

# Perfectionism in the Public Domain: a Natrual Language Processing Approach
<span style="color:grey">Michael Page</span>
<br/>  

### 1. ABSTRACT
***

### 2. INTRODUCTION
***

##### 2.1 Introduction to the problem

Recent meta-analytical evidence has demonstrated that levels of perfectionism in Western populations has linearly increased over the past three decades [(Curran & Hill, 2017)](https://www.apa.org/pubs/journals/releases/bul-bul0000138.pdf). This has coincided with a rapid growth in the number of research articles investigating the outcomes, processes, and characteristics associated with perfectionism since the introduction of the first multidimensional perfectionism measures in the early 90's. Despite a growing body of perfectionism literature in the academic domain, little is known (from the perspective of the academic) regarding reporting standards of perfectionism in the public domain. Indeed, it is important that academic research is *accurately* translated and disseminated to the broader public. Nonetheless, the extent to which this holds true in the realm of perfectionism is unknown.

##### 2.2 Perfectionism

Broadly defined, perfectionism is understood to be a multidimensional personality trait consisting of two higher-order dimensions: perfectionistic strivings and perfectionistic concerns [(Stoeber & Otto, 2006)](http://journals.sagepub.com/doi/10.1207/s15327957pspr1004_2). Perfectionistic strivings capture the setting of high performance standards and self- oriented strivings for perfection, whereas perfectionistic concerns capture the negative reactions to imperfections and mistakes, and the fear of negative social appraisal [(Gotwals, Stoeber, Dunn, and Stoll, 2012)](http://psycnet.apa.org/buy/2012-33135-003). These two dimensions are considered to be part of a hierarchical model or heuristic representative of a range of different models that exists (Hill, 2016). Multiple reviews support that both perfectionistic strivings and perfectionistic concerns display a typical pattern of findings: perfectionistic strivings are associated with adaptive outcomes, processes and characteristics, whereas perfectionistic concerns are associated with maladaptive outcomes, processes, and characteristics (e.g., Gotwals et al., 2012, Stoeber, 2011, Stoeber & Otto, 2006).

##### 2.3 Framing the problem

An assessment of the meaning and social understanding of perfectionism, both in regards to whether it is perceived as a positive and/or negative trait (in line with the research on perfectionistic strivings and perfectionistic concerns), and inferred by the other words/topics it coalesces with, would contribute to research in this area. Fortunately, a wealth of natural language processing tools are at the researchers disposal to answer such questions. These include sentiment analyses and machine learning methods such as topic modelling. The purpose of this project is, therefore, to employ a variety of natural language processing techniques to better understand how perfectionism is reported in the public domain.

### 3. DATA WRANGLING
***

##### 3.1 NewsRiver API client
To obtain data, an API client for the [NewsRiver API](https://newsriver.io) was built using the 'httr' package. Code for the API client (alongside code for the whole project) can be found in the appendix (it should be noted that where relevant, code is also included in text). The NewsRiver API was selected for its large library of news sources and broad range of programmable search parameters. Accordingly, the API client was designed to accept a variety of search parameters from search dates to title and text keyword searches (among others):

```{r API parameters, echo = TRUE, eval = FALSE}
# Set date range for api search

search_dates <- seq(as.Date("2017-07-01"), as.Date("2018-07-01"), by = "months")

# Set query parameters to be called

query <- sprintf('title:("perfectionism" OR "perfect") AND text:"perfectionism" AND language:en AND discoverDate:[%s TO %s]', search_dates, search_dates %m+% months(1))

```

##### 3.2 Wrangling JSON files
The API returned a JSON file which was then parsed as text and stored in a dataframe object (in this instance as a tibbles). The returned tibble contained 19 different variables, many of which contained metadata and non-essential information (e.g., read time, website icon url, etc.). Moreover, many of these non-essential variables contained large quantities of NA values. Subsequently, four key variables of interest (title, text, date, and website) were selected inline with the aims of the project. The website and article publication date variables were renamed, and the article publication date variable was parsed into date format:

```{r wrangling JSON files, echo = TRUE, eval = FALSE}
# Return error and stop execution if a json is not returned

	if (http_type(resp) != "application/json") {
    stop("API did not return json", call. = FALSE)
	}

	# Return error if there is a http error, else parse the content from the json file and store the title, text, date, and website in a tibble

	if (http_error(resp) == TRUE) {
		warning("The request failed")
	} else {
		news_tbl <- fromJSON(content(resp, as = "text", encoding = "UTF-8"), flatten = TRUE)  %>%
		as_tibble()

		if (nrow(news_tbl) != 0) {
			news_tbl <- news_tbl %>% mutate(date = as.Date(discoverDate), website = website.domainName) %>% select(title, text, date, website)
		}

```

##### 3.3 Cleaning variables

The next step in the data wrangling process involved cleaning the selected variables. The trasnformed tibble was searched for NA values, of which the website variable was found to contain three. As the website variable was not needed for all analyses, these observations were kept in order to maintain sample size.

After parsing the JSON files, many unicode characters (e.g., "i\u2019m") were found in the text and title variable data and were transformed into ASCII characters using the 'stringi' package:

```{r unicode characters, echo = TRUE, eval = FALSE}
perf_news %<>% mutate(text = stringi::stri_trans_general(perf_news$text, "latin-ascii"), title = stringi::stri_trans_general(perf_news$title, "latin-ascii"))
```

Next, the tibble was searched for duplicate obersvations. Strings from the title column were transformed to lower case and then duplicate observations were then removed using the distinct function from 'dplyr':

```{r duplicate observations, echo = TRUE, eval = FALSE}
perf_news %<>% mutate(title = str_to_lower(title))
perf_news %<>% distinct(title, .keep_all = TRUE)
```

Finally, the tibble was manually inspected for errors and subsequent observations were removed:

```{r manual error search, echo = TRUE, eval = FALSE}
perf_news <- perf_news %>%
  filter(!(perf_news$title == perf_news$title[52])) %>% 
  filter(!(perf_news$title == perf_news$title[3]))
```

##### 3.4 Tidy text

For the majority of analyses in this project, a tidy data structure was used. A tidy data structure is one where each variable is a column, each observation is a row, and each type of observational unit is a table [(Wickham, 2014)](https://www.jstatsoft.org/article/view/v059i10). Accordingly, the cleaned tibbles above were tokenised into a 'tidy text' format, with one word per row as specificed in [Silge & Robinson (2017)](https://www.tidytextmining.com):

```{r tidy text, echo = TRUE, eval = FALSE}
tidy_news <- perf_news %>% unnest_tokens(word, text)
```

Finally, stop words (e.g., "the", "to", "of", etc.) were removed:

```{r stop words, echo = TRUE, eval = FALSE}
tidy_news <- tidy_news %>% anti_join(stop_words)
```

### 4. DATA SETS

The wrangled data set, tidy_news, contained 18,160 observations, split across 74 news articles from a variety of sources. Each article contained the word 'perfectionism' or 'perfect' in the title and 'perfectionism' in the text on at least one occasion, as specified in the search paramaters in section 3.1. The date range of the articles was from 2017-11-22 to 2018-07-03 (these limits were imposed by the API):

```{r data set}
tidy_news <- readr::read_rds("tidy_news.RDS")

knitr::kable(tidy_news[17585:17595,])
```

It should be noted that although the tidy_news data set formed the basic data stucture on which an array of analyses were performed, the data set was further wrangled into several other forms. This indluded a tidy data set tokenised by sentence, and a document-term matrix (among others). In the interest of conciseness, examples of these data sets have been omitted from this report, however, code for these data set transformations can be found in the appendices.

### 5. ANALYSES
***

```{r preparatory code, include = FALSE}

# Load libraries

library(tidyverse)
library(httr)
library(jsonlite)
library(xml2)
library(urltools)
library(lubridate)
library(magrittr)
library(tidytext)
library(tidyr)
library(wordcloud)
library(reshape2)
library(igraph)
library(ggraph)
library(widyr)
library(topicmodels)
library(ldatuning)

# Load data sets

perf_news <- read_rds("perf_news.RDS")
tidy_news <- read_rds("tidy_news.RDS")

```

##### 5.1 Frequency distributions

In order to understand the underlying structure of the data, several exploratory data analyses were performed.  

*Term frequency distribution*: the frequency of terms in each article were calculated and then divided by the total number of terms in each article to provide a measure of term frequency, as can be seen in Figure 1:

```{r term frequency distribution}

words_news <- perf_news %>% 
  unnest_tokens(word, text) %>% 
  count(title, word, sort = TRUE) %>% 
  ungroup()

words_total <- words_news %>% 
  group_by(title) %>% 
  summarise(total = sum(n))

words_news <- left_join(words_news, words_total)

ggplot(words_news, aes(n/total, fill = title)) +
  geom_histogram(show.legend = FALSE) +
  xlim(NA, 0.025) +
  theme(strip.text = element_text(size = 7))

```

The distribution in Figure 1 deomnstrates a large positive skew, as would be expected in a corpus of natural language (as some types of words such as articles, appear more frequently than other types of words). 

*Zipf's law*: to further explore the plotted term frequency distribtuion, a second plot of frequency rank on the x-axis and term frequency on the y-axis, on logarithmic scales, was created. Moreover, a linear model was fitted to the plot to further examine its underlying structure (Figure 2):

```{r linear model, include = FALSE}

freq_by_rank <- words_news %>% 
  group_by(title) %>% 
  mutate(rank = row_number(), `term frequency` = n/total)

lm(log10(`term frequency`) ~ log10(rank), data = freq_by_rank)

```

```{r Zipfs law}

 freq_by_rank %>% 
  ggplot(aes(rank, `term frequency`, color = title)) +
  geom_abline(intercept = -1.1029,slope = -0.7564, color = "black", linetype = 2) +
  geom_line(size = 0.5, alpha = 0.5, show.legend = FALSE) +
  scale_x_log10() +
  scale_y_log10()

```

The plot shown in Figure 2 further demonstrates that the corpus of text in the perf_news data set demonstrates a typical structure for a corpus of natural language. The type of relationship shown in Figure 2 resembles that of Zipf's law (an interpretation of the power law) in that there is an inverse relationship between term frequency and rank. In a perfect case of Zipf's law the plot would deomnstrate a perfect negative slope (-1). In the perf_news data set, however, there are small deviations at both the higher and lower ranked words. This means that the data set contains fewer rare words than would typically be predicted by a power law. Furthermore, the data set also appears to contain fewer common words than one may expect. Nonetheless, the plots in Figure 1 and Figure 2 both support that the perf_news data set represents a typical corpus of natural language, justfying further analyses that operate on the assumption that the distriubtion of words is typical.

##### Word frequencies

```{r word frequencies bar}
 
# Create custom stop words

custom_stop_words <-  bind_rows(tibble(word = c("perfect", 
                                                "perfection", 
                                                "perfectionism", 
                                                "perfectly", 
                                                "perfectionist", 
                                                "perfectionists", 
                                                "curran", 
                                                "thomas", 
                                                "andy", 
                                                "hill"), 
                                       lexicon = c("custom")), stop_words)

# Find and plot the most common words in tidy_news after applying custom stop words

# As a bar chart:

tidy_news %>%
  anti_join(custom_stop_words) %>% 
  count(word, sort = TRUE) %>% 
  filter(n > 50) %>% 
  mutate(word = reorder(word, n)) %>% 
  ggplot(aes(word, n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip() +
  labs(y = "Word Frequency (n)")

```


```{r word frequencies bar cloud}

# As a word cloud:

tidy_news %>%
  anti_join(custom_stop_words) %>% 
  count(word) %>% 
  with(wordcloud(word, n, max.words = 100, colors= c("steelblue1","steelblue2","steelblue3","steelblue")))

```


Immediately, it can be observed that words regarding college and mental health appear frequently in the texts - topics one would expect to appear in academic literature regarding perfectionism. This preliminary frequnecy analysis supports that the datat may be representive of research in the academic domain and warrant further investigation.


##### Sentiment Analyses

*Sentiment over time*: Next, articles were tokenised by word to create unigrams, on which sentiment analyses over time were performed using the AFINN, Bing, and NRC lexicons. Discuss about how the articles were analysed in sentene length, by summing words across whole sentence, as opposed to as just individual words. This is because it is better reflects the english language, where typically, one sentence tries to exhibit one point. Thereform the sentiment for each point can be captured.

```{r tidier news, include = FALSE}

# Create a new data set called tidier_news which is tokenized by word, but keeps track of sentence number

tidier_news <- perf_news %>% 
  unnest_tokens(sentence, text, token = "sentences") %>% 
  group_by(title) %>% 
  mutate(sentence_number = row_number()) %>% 
  ungroup() %>%
  unnest_tokens(word, sentence) %>% 
  anti_join(custom_stop_words)

```


```{r sentiment analyses}

# Perform sentiment analyses using three different sentiment lexicons (AFINN, Bing, and NRC). Compute sentiment in sentence units by summing individual word sentiment scores across each sentence.

# AFINN

afinn <- tidier_news %>% 
  inner_join(get_sentiments("afinn")) %>%
  group_by(title, sentence_number) %>% 
  mutate(sentiment = sum(score)) %>%
  select(date, title, sentence_number, sentiment) %>% 
  distinct() %>% 
  group_by(title) %>% 
  mutate(sent_sum = sum(sentiment)) %>% 
  ungroup() %>% 
  select(date, title, sent_sum) %>% 
  distinct()

# Bing

bing <- tidier_news %>% 
  inner_join(get_sentiments("bing")) %>% 
  count(date, title, sentence_number, sentiment) %>% 
  spread(sentiment, n, fill = 0) %>% 
  mutate(sentiment = positive - negative) %>% 
  group_by(title) %>% 
  mutate(sent_sum = sum(sentiment)) %>% 
  ungroup() %>% 
  select(date, title, sent_sum) %>% 
  distinct()

# NRC

nrc <- tidier_news %>% 
  inner_join(get_sentiments("nrc")) %>% 
  filter(sentiment %in% c("positive", "negative")) %>%
  count(date, title, sentence_number, sentiment) %>% 
  spread(sentiment, n, fill = 0) %>% 
  mutate(sentiment = positive - negative) %>% 
  group_by(title) %>% 
  mutate(sent_sum = sum(sentiment)) %>% 
  ungroup() %>% 
  select(date, title, sent_sum) %>% 
  distinct()

# Plot all three sentiment analyses on one graph

bind_rows(afinn %>% mutate(method = "AFINN"), bing %>% mutate(method = "Bing et al."), nrc %>% mutate(method = "NRC")) %>% 
  ggplot(aes(date, sent_sum, fill = method)) +
  geom_col(position = position_dodge(0.5), show.legend = FALSE) +
  facet_wrap(~method, ncol = 1, scales = "free_y") +
  labs(x = "Date", y = "Sentiment Score")

```

*Frequent positive and negative words*: the most frequent positive and negative words across all texts were found using the Bing sentiment lexicon and plotted as a bar chart: 

```{r positive negative bar}

# Find the most common positive and negative words

bing_word_counts <- tidier_news %>% 
  inner_join(get_sentiments("bing")) %>% 
  count(word, sentiment, sort = TRUE) %>% 
  ungroup()

# Plot the most common positive and negative words

# As a bar chart:

bing_word_counts %>% 
  group_by(sentiment) %>% 
  top_n(10) %>% 
  ungroup() %>% 
  mutate(word = reorder(word, n)) %>% 
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(y = "contribution to sentiment", x = NULL) +
  coord_flip()

```

```{r positive negative cloud}

# As a word cloud:

tidier_news %>% 
  inner_join(get_sentiments("bing")) %>% 
  count(word, sentiment, sort = TRUE) %>% 
  acast(word ~ sentiment, value.var = "n", fill = 0) %>% 
  comparison.cloud(colors = c("gray8", "darkorange"), max.words = 100)

```


##### Bigrams

While there is some merit in a unigram/single-words approach (find tech definition), bigrams have an advanagtage ...

```{r bigram prep, include = FALSE}

# Unnest tokens by bigrams keeping track of sentence number

bigram_news <- perf_news %>%
  unnest_tokens(sentence, text, token = "sentences") %>% 
  group_by(title) %>% 
  mutate(sentence_number = row_number()) %>% 
  ungroup() %>% 
  unnest_tokens(bigram, sentence, token = "ngrams", n = 2)

# Separate bigrams into two columns, "word1", and "word2".

bigrams_separated <- bigram_news %>% 
  separate(bigram, c("word1", "word2"), sep = " ")

```

*Sentiment analysis*: 

```{r bigram sentiment}

# Use bigrams to perform sentiment analyses by reversing the sentiment score of negated words

AFINN <- get_sentiments("afinn")

negation_words <- c("not", "never", "no", "without")

bigrams_afinn <- bigrams_separated %>% 
  filter(!word1 %in% custom_stop_words$word) %>% 
  filter(!word2 %in% custom_stop_words$word) %>% 
  inner_join(AFINN, by = c(word2 = "word")) %>%
  mutate(score = ifelse(word1 %in% negation_words, -score, score))

bigrams_afinn_sentiment <- bigrams_afinn %>%
  group_by(title, sentence_number) %>% 
  mutate(sentiment = sum(score)) %>%
  select(date, title, sentence_number, sentiment) %>% 
  distinct() %>% 
  group_by(title) %>% 
  mutate(sent_sum = sum(sentiment)) %>% 
  ungroup() %>% 
  select(date, title, sent_sum) %>% 
  distinct()

ggplot(bigrams_afinn_sentiment, aes(date, sent_sum, fill = date)) +
  geom_col(position = position_dodge(0.7), width = 2, show.legend = FALSE)

```

*Bigram frequency*: 

```{r bigram frequency}

# Count most frequent bigrams not keeping track of sentence

bigram_counts <- perf_news %>% 
  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>% 
  separate(bigram, c("word1", "word2"), sep = " ") %>% 
  filter(!word1 %in% stop_words$word) %>% 
  filter(!word2 %in% stop_words$word) %>% 
  count(word1, word2, sort = TRUE)

knitr::kable(bigram_counts[1:10, ])


```

*Bigram igraph* REMOVE LARGE DOTS ETC.? Go back to basic graph in tidytext. Change code here and in capstone_code.R file.

```{r bigram igraph}

# Create igraph object for the most frequent bigrams. The transparency of links denotes how common or rare the bigram is

bigram_graph <- bigram_counts %>% 
  filter(n > 6) %>% 
  graph_from_data_frame()

set.seed(1234)
a <- grid::arrow(type = "closed", length = unit(.05, "inches"))
ggraph(bigram_graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE, arrow = a, end_cap = circle(.07, 'inches')) +
  geom_node_point(color = "lightblue", size = 3) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  theme_void()

```

##### Pairwise correlations

```{r pairwise prep, include = FALSE}

# Calculate pairwise correlations of words using the phi coefficient. This identifies how much more likely it is that either both word X and Y appear, or neither do, than that one appears without the other.

word_cors <- perf_news %>% 
  unnest_tokens(sentence, text, token = "sentences") %>% 
  group_by(title) %>% 
  mutate(sentence_number = row_number()) %>% 
  ungroup() %>%
  unnest_tokens(word, sentence) %>% 
  filter(!word %in% stop_words$word) %>% 
  group_by(word) %>% 
  filter(n() >= 20) %>% 
  pairwise_cor(word, title, sort = TRUE)

```

```{r pairwise cor}

# Plot words most correlated with different perfectionism terms

word_cors %>%
  filter(item1 %in% c("perfect", "perfection", "perfectionism", "perfectly", "perfectionist", "perfectionists")) %>% 
  group_by(item1) %>%
  top_n(6) %>%
  ungroup() %>% 
  group_by(item1, item2) %>%                  
  arrange(desc(correlation)) %>%                
  ungroup() %>%
  mutate(item2 = factor(paste(item2, item1, sep = "__"), levels = rev(paste(item2, item1, sep = "__")))) %>%
  ggplot(aes(item2, correlation, fill = item1)) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  facet_wrap(~ item1, scales = "free") +
  coord_flip() +
  scale_x_discrete(labels = function(x) gsub("__.+$", "", x)) +
  xlab(NULL)

```

##### Topic models

```{r dtm, include = FALSE, cache = TRUE}

# Create Document Term Matrix

news_dtm <- tidy_news %>% 
  anti_join(custom_stop_words) %>%
  count(title, word) %>% 
  cast_dtm(title, word, n)

# Select number of topics (k) for LDA model using the 'ldatuninig' package.

lda_fit <-FindTopicsNumber(news_dtm,
                           topics = seq(from = 2, to = 50, by = 1),
                           metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"),
                           method = "Gibbs", control = list(seed = 77), mc.cores = 2L, verbose = TRUE)

```

```{r lda fit graph, fig.width = 12}

# find the extremum to determine optimal k

FindTopicsNumber_plot(lda_fit)

```

```{r fit topic models, include = FALSE}

# Fit topic models using latent Dirichlet allocation

perf_lda <- LDA(news_dtm, k = 9, control = list(seed = 1234))

# Extract the per-topic-per-word probabilities (beta).

perf_topics <- tidy(perf_lda, matrix = "beta")

```

```{r}

# Find most common terms for each topic.

perf_top_terms <-  perf_topics %>% 
  group_by(topic) %>% 
  top_n(5, beta) %>% 
  ungroup() %>% 
  arrange(topic, -beta)

perf_top_terms %>% 
  group_by(topic, term) %>%                  
  arrange(desc(beta)) %>%                
  ungroup() %>%
  mutate(term = factor(paste(term, topic, sep = "__"), levels = rev(paste(term, topic, sep = "__")))) %>%
  mutate(term = reorder(term, beta)) %>% 
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip() +
  scale_x_discrete(labels = function(x) gsub("__.+$", "", x)) +
  xlab(NULL)
```

### 6. DISCUSSION
***

Limitations: articles restricted to 2018. Single word and bigrams limited. Tokeniser limitations.

### 7. RECOMMENDATIONS
***


for client: 3
for future research: use sentimentr package and analyse by sentence structure (See github page for tech deets). In discussion or this section? See goran meeting notes for other analysis and critiques

### 8. REFERENCES
### 9. APPENDIX
***

```{r capstone code, echo = TRUE, eval = FALSE}

# --------------------
# LIBRARIES
# --------------------

# Load libraries

library(tidyverse)
library(httr)
library(jsonlite)
library(xml2)
library(urltools)
library(lubridate)
library(magrittr)
library(tidytext)
library(tidyr)
library(wordcloud)
library(reshape2)
library(igraph)
library(ggraph)
library(widyr)
library(topicmodels)
library(ldatuning)

# --------------------
# DATA WRANGLING
# --------------------

# Create Newsriver API

# Create function to retrieve data based on query paramters. See 'API REFERENCE' on newsriver.io for a list of query parameters. The function returns a tibble containing the title, text, date, and website relating to the query parameters.

newsriver_api <- function(query) {
  
  # Set rate limit and show progress. The API rate limit is 25 calls per window per API token. The rate limiting window is 15 minutes long.
  
  p$tick()$print()
  Sys.sleep(37)
  
  # Create URL encoded base to be passed custom query parameters
  
  url_base <- "https://api.newsriver.io/v2/search" %>% 
    param_set("query", url_encode(query)) %>% 
    param_set("sortBy", "_score") %>% 
    param_set("sortOrder", "DESC") %>% 
    param_set("limit", "100")
  
  # Make GET request
  
  resp <- GET(url_base, ua, add_headers(Authorization = api_token))
  
  # Return error and stop execution if a json is not returned
  
  if (http_type(resp) != "application/json") {
    stop("API did not return json", call. = FALSE)
  }
  
  # Return error if there is a http error, else parse the content from the json file and store the title, text, date, and website in a tibble
  
  if (http_error(resp) == TRUE) {
    warning("The request failed")
  } else {
    news_tbl <- fromJSON(content(resp, as = "text", encoding = "UTF-8"), flatten = TRUE)  %>%
      as_tibble()
    
    if (nrow(news_tbl) != 0) {
      news_tbl <- news_tbl %>% mutate(date = as.Date(discoverDate), website = website.domainName) %>% select(title, text, date, website)
    }
    
    news_tbl
  }
}

# Set user agent and api token

ua <- user_agent("inser user agent here")
api_token <-  # "insert token here"
  
# Set date range for api search
  
search_dates <- seq(as.Date("2017-07-01"), as.Date("2018-07-01"), by = "months")

# Set query parameters to be called

query <- sprintf('title:("perfectionism" OR "perfect") AND text:"perfectionism" AND language:en AND discoverDate:[%s TO %s]', search_dates, search_dates %m+% months(1))

# Initialise progress bar

p <- progress_estimated(length(search_dates))

# Call newsriver_api function over the vector of query parameters and return a tibble

perf_news <- map_dfr(query, newsriver_api)

# Remove unicode characters from title and text

perf_news %<>% mutate(text = stringi::stri_trans_general(perf_news$text, "latin-ascii"), title = stringi::stri_trans_general(perf_news$title, "latin-ascii"))

# Convert titles to lower case to enable duplicate detection

perf_news %<>% mutate(title = str_to_lower(title))

# Remove duplicates

perf_news %<>% distinct(title, .keep_all = TRUE)

# Inspect each article and remove any errors or unrelated texts

perf_news <- perf_news %>%
  filter(!(perf_news$title == perf_news$title[52])) %>% 
  filter(!(perf_news$title == perf_news$title[3]))

# Unnest perf_news text column

tidy_news <- perf_news %>% unnest_tokens(word, text)

# Remove stop words

tidy_news <- tidy_news %>% anti_join(stop_words)

# --------------------
# DATA ANALYSIS
# --------------------

# Examine the distribution of words in the data set. First, calculate the term frequency of each term in each article, then divide by the total number of terms in that article. Finally, plot the results.

words_news <- perf_news %>% 
  unnest_tokens(word, text) %>% 
  count(title, word, sort = TRUE) %>% 
  ungroup()

words_total <- words_news %>% 
  group_by(title) %>% 
  summarise(total = sum(n))

words_news <- left_join(words_news, words_total)

ggplot(words_news, aes(n/total, fill = title)) +
  geom_histogram(show.legend = FALSE) +
  xlim(NA, 0.025) +
  theme(strip.text = element_text(size = 7))

# To further examine the structure of the data, fit and plot a linear model to freq_by_rank to demonstrate Zipf's law has been maintained.

freq_by_rank <- words_news %>% 
  group_by(title) %>% 
  mutate(rank = row_number(), `term frequency` = n/total)

lm(log10(`term frequency`) ~ log10(rank), data = freq_by_rank)

freq_by_rank %>% 
  ggplot(aes(rank, `term frequency`, color = title)) +
  geom_abline(intercept = -1.1029,slope = -0.7564, color = "black", linetype = 2) +
  geom_line(size = 0.5, alpha = 0.5, show.legend = FALSE) +
  scale_x_log10() +
  scale_y_log10()

# Create custom stop words

custom_stop_words <-  bind_rows(tibble(word = c("perfect", 
                                                "perfection", 
                                                "perfectionism", 
                                                "perfectly", 
                                                "perfectionist", 
                                                "perfectionists", 
                                                "curran", 
                                                "thomas", 
                                                "andy", 
                                                "hill"), 
                                       lexicon = c("custom")), stop_words)

# Find and plot the most common words in tidy_news after applying custom stop words

# As a bar chart:

tidy_news %>%
  anti_join(custom_stop_words) %>% 
  count(word, sort = TRUE) %>% 
  filter(n > 50) %>% 
  mutate(word = reorder(word, n)) %>% 
  ggplot(aes(word, n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip() +
  labs(y = "Word Frequency (n)")

# As a word cloud:

tidy_news %>%
  anti_join(custom_stop_words) %>% 
  count(word) %>% 
  with(wordcloud(word, n, max.words = 100, colors= c("steelblue1","steelblue2","steelblue3","steelblue")))

# Create a new data set called tidier_news which is tokenized by word, but keeps track of sentence number

tidier_news <- perf_news %>% 
  unnest_tokens(sentence, text, token = "sentences") %>% 
  group_by(title) %>% 
  mutate(sentence_number = row_number()) %>% 
  ungroup() %>%
  unnest_tokens(word, sentence) %>% 
  anti_join(custom_stop_words)

# Perform sentiment analyses using three different sentiment lexicons (AFINN, Bing, and NRC). Compute sentiment in sentence units by summing individual word sentiment scores across each sentence.

# AFINN

afinn <- tidier_news %>% 
  inner_join(get_sentiments("afinn")) %>%
  group_by(title, sentence_number) %>% 
  mutate(sentiment = sum(score)) %>%
  select(date, title, sentence_number, sentiment) %>% 
  distinct() %>% 
  group_by(title) %>% 
  mutate(sent_sum = sum(sentiment)) %>% 
  ungroup() %>% 
  select(date, title, sent_sum) %>% 
  distinct()

# Bing

bing <- tidier_news %>% 
  inner_join(get_sentiments("bing")) %>% 
  count(date, title, sentence_number, sentiment) %>% 
  spread(sentiment, n, fill = 0) %>% 
  mutate(sentiment = positive - negative) %>% 
  group_by(title) %>% 
  mutate(sent_sum = sum(sentiment)) %>% 
  ungroup() %>% 
  select(date, title, sent_sum) %>% 
  distinct()

# NRC

nrc <- tidier_news %>% 
  inner_join(get_sentiments("nrc")) %>% 
  filter(sentiment %in% c("positive", "negative")) %>%
  count(date, title, sentence_number, sentiment) %>% 
  spread(sentiment, n, fill = 0) %>% 
  mutate(sentiment = positive - negative) %>% 
  group_by(title) %>% 
  mutate(sent_sum = sum(sentiment)) %>% 
  ungroup() %>% 
  select(date, title, sent_sum) %>% 
  distinct()

# Plot all three sentiment analyses on one graph

bind_rows(afinn %>% mutate(method = "AFINN"), bing %>% mutate(method = "Bing et al."), nrc %>% mutate(method = "NRC")) %>% 
  ggplot(aes(date, sent_sum, fill = method)) +
  geom_col(position = position_dodge(0.5), show.legend = FALSE) +
  facet_wrap(~method, ncol = 1, scales = "free_y") +
  labs(x = "Date", y = "Sentiment Score")

# Find the most common positive and negative words

bing_word_counts <- tidier_news %>% 
  inner_join(get_sentiments("bing")) %>% 
  count(word, sentiment, sort = TRUE) %>% 
  ungroup()

# Plot the most common positive and negative words

# As a bar chart:

bing_word_counts %>% 
  group_by(sentiment) %>% 
  top_n(10) %>% 
  ungroup() %>% 
  mutate(word = reorder(word, n)) %>% 
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(y = "contribution to sentiment", x = NULL) +
  coord_flip()

# As a word cloud:

tidier_news %>% 
  inner_join(get_sentiments("bing")) %>% 
  count(word, sentiment, sort = TRUE) %>% 
  acast(word ~ sentiment, value.var = "n", fill = 0) %>% 
  comparison.cloud(colors = c("gray8", "darkorange"), max.words = 100)

# Unnest tokens by bigrams keeping track of sentence number

bigram_news <- perf_news %>%
  unnest_tokens(sentence, text, token = "sentences") %>% 
  group_by(title) %>% 
  mutate(sentence_number = row_number()) %>% 
  ungroup() %>% 
  unnest_tokens(bigram, sentence, token = "ngrams", n = 2)

# Separate bigrams into two columns, "word1", and "word2".

bigrams_separated <- bigram_news %>% 
  separate(bigram, c("word1", "word2"), sep = " ")

# Use bigrams to perform sentiment analyses by reversing the sentiment score of negated words

AFINN <- get_sentiments("afinn")

negation_words <- c("not", "never", "no", "without")

bigrams_afinn <- bigrams_separated %>% 
  filter(!word1 %in% custom_stop_words$word) %>% 
  filter(!word2 %in% custom_stop_words$word) %>% 
  inner_join(AFINN, by = c(word2 = "word")) %>%
  mutate(score = ifelse(word1 %in% negation_words, -score, score))

bigrams_afinn_sentiment <- bigrams_afinn %>%
  group_by(title, sentence_number) %>% 
  mutate(sentiment = sum(score)) %>%
  select(date, title, sentence_number, sentiment) %>% 
  distinct() %>% 
  group_by(title) %>% 
  mutate(sent_sum = sum(sentiment)) %>% 
  ungroup() %>% 
  select(date, title, sent_sum) %>% 
  distinct()

ggplot(bigrams_afinn_sentiment, aes(date, sent_sum, fill = date)) +
  geom_col(position = position_dodge(0.7), width = 2, show.legend = FALSE)

# Count most frequent bigrams not keeping track of sentence

bigram_counts <- perf_news %>% 
  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>% 
  separate(bigram, c("word1", "word2"), sep = " ") %>% 
  filter(!word1 %in% stop_words$word) %>% 
  filter(!word2 %in% stop_words$word) %>% 
  count(word1, word2, sort = TRUE)

# Count most frequent trigrams not keeping track of sentence

perf_news %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 3) %>% 
  separate(bigram, c("word1", "word2", "word3"), sep = " ") %>% 
  filter(!word1 %in% stop_words$word,
         !word2 %in% stop_words$word,
         !word3 %in% stop_words$word) %>% 
  count(word1, word2, word3, sort = TRUE)


# Create igraph object for the most frequent bigrams. The transparency of links denotes how common or rare the bigram is

bigram_graph <- bigram_counts %>% 
  filter(n > 5) %>% 
  graph_from_data_frame()

set.seed(1234)
a <- grid::arrow(type = "closed", length = unit(.15, "inches"))
ggraph(bigram_graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE, arrow = a, end_cap = circle(.07, 'inches')) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  theme_void()
 
# Calculate pairwise correlations of words using the phi coefficient. This identifies how much more likely it is that either both word X and Y appear, or neither do, than that one appears without the other.

word_cors <- perf_news %>% 
  unnest_tokens(sentence, text, token = "sentences") %>% 
  group_by(title) %>% 
  mutate(sentence_number = row_number()) %>% 
  ungroup() %>%
  unnest_tokens(word, sentence) %>% 
  filter(!word %in% stop_words$word) %>% 
  group_by(word) %>% 
  filter(n() >= 20) %>% 
  pairwise_cor(word, title, sort = TRUE)

# Plot words most correlated with different perfectionism terms

word_cors %>%
  filter(item1 %in% c("perfect", "perfection", "perfectionism", "perfectly", "perfectionist", "perfectionists")) %>% 
  group_by(item1) %>%
  top_n(6) %>%
  ungroup() %>% 
  group_by(item1, item2) %>%                  
  arrange(desc(correlation)) %>%                
  ungroup() %>%
  mutate(item2 = factor(paste(item2, item1, sep = "__"), levels = rev(paste(item2, item1, sep = "__")))) %>%
  ggplot(aes(item2, correlation, fill = item1)) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  facet_wrap(~ item1, scales = "free") +
  coord_flip() +
  scale_x_discrete(labels = function(x) gsub("__.+$", "", x)) +
  xlab(NULL)

# Create Document Term Matrix

news_dtm <- tidy_news %>% 
  anti_join(custom_stop_words) %>%
  count(title, word) %>% 
  cast_dtm(title, word, n)

# Select number of topics (k) for LDA model using the 'ldatuninig' package.

lda_fit <-FindTopicsNumber(news_dtm,
                           topics = seq(from = 2, to = 50, by = 1),
                           metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"),
                           method = "Gibbs", control = list(seed = 77), mc.cores = 2L, verbose = TRUE)

# find the extremum to determine optimal k

FindTopicsNumber_plot(lda_fit)

# Fit topic models using latent Dirichlet allocation

perf_lda <- LDA(news_dtm, k = 9, control = list(seed = 1234))

# Extract the per-topic-per-word probabilities (beta).

perf_topics <- tidy(perf_lda, matrix = "beta")

# Find most common terms for each topic.

perf_top_terms <-  perf_topics %>% 
  group_by(topic) %>% 
  top_n(5, beta) %>% 
  ungroup() %>% 
  arrange(topic, -beta)

perf_top_terms %>% 
  group_by(topic, term) %>%                  
  arrange(desc(beta)) %>%                
  ungroup() %>%
  mutate(term = factor(paste(term, topic, sep = "__"), levels = rev(paste(term, topic, sep = "__")))) %>%
  mutate(term = reorder(term, beta)) %>% 
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip() +
  scale_x_discrete(labels = function(x) gsub("__.+$", "", x)) +
  xlab(NULL)

# Extract the per-document-per-topic probabilities (gamma).

perf_documents <- tidy(perf_lda, matrix = "gamma")


# Find which words in each document were assigned to which topic.

assignments <- augment(perf_lda, data = news_dtm)

```
ADD FIGURE CAPTIONS