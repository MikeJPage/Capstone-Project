---
title: "An Introduction to Machine Learning"
author: "Michael Page"
date: "05/08/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, fig.align = "center")
```

```{r preparatory code, include = FALSE}

# Load libraries

library(tidyverse)
library(httr)
library(jsonlite)
library(xml2)
library(urltools)
library(lubridate)
library(magrittr)
library(tidytext)
library(tidyr)
library(wordcloud)
library(reshape2)
library(igraph)
library(ggraph)
library(widyr)
library(topicmodels)
library(ldatuning)

# Load data sets

perf_news <- read_rds("perf_news.RDS")
tidy_news <- read_rds("tidy_news.RDS")

# Create custom stop words

custom_stop_words <-  bind_rows(tibble(word = c("perfect", 
                                                "perfection", 
                                                "perfectionism", 
                                                "perfectly", 
                                                "perfectionist", 
                                                "perfectionists", 
                                                "curran", 
                                                "thomas", 
                                                "andy", 
                                                "hill"), 
                                       lexicon = c("custom")), stop_words)

```



##### 4.6 Topic models

*Model fit*: one question of interest is what topics perfectionism coalesce with in the collected news articles. In order to answer such a question the data must be divided into a set of natural groups. One such method for achieving this is topic modelling, a method of unsupervised classification of documents. In topic modelling, each documented is treated as a mixture of topics, and each topic as a mixture of words. This prevents documents being categorised into discrete groups, and allows for overlap in terms of content in a way that represents the structure of natural language. Consequently, topic models were fitted to the data. First, a document-term matrix was calculated and then the 'ldatuning' package was used to determine the optimum number of topics (*k*). The results from this optimisation process can be observed in Figure 10:

```{r dtm, include = FALSE, cache = TRUE}

# Create Document Term Matrix

news_dtm <- tidy_news %>% 
  anti_join(custom_stop_words) %>%
  count(title, word) %>% 
  cast_dtm(title, word, n)

# Select number of topics (k) for LDA model using the 'ldatuninig' package.

lda_fit <-FindTopicsNumber(news_dtm,
                           topics = seq(from = 2, to = 50, by = 1),
                           metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"),
                           method = "Gibbs", control = list(seed = 77), mc.cores = 2L, verbose = TRUE)

```

```{r lda fit graph, fig.width = 12, fig.cap='Figure 10. Topic model fit according to the number of topics.'}

# find the extremum to determine optimal k

FindTopicsNumber_plot(lda_fit)

```

The results in Figure 10 indicate the optimum number of topics (*k*) occurs in the range 6-15, as indicated by the variance in extremum. Subsequently, topic models were fitted incrementally in this range and qualitatively evaluated at each stage. In this instance, the optimum number of topics occurred at *k* = 9.



```{r fit topic models, include = FALSE}

# Fit topic models using latent Dirichlet allocation

perf_lda <- LDA(news_dtm, k = 9, control = list(seed = 1234))

# Extract the per-topic-per-word probabilities (beta).

perf_topics <- tidy(perf_lda, matrix = "beta")

```

*Word-topic probabilities*: having established the number of topics (*k* = 9), the per-topic-per-word probabilities ("beta") were then calculated. The beta probabilities demonstrate the most common words within topics, as demonstrated in Figure 11:

```{r topic model terms, fig.cap='Figure 11. Word-topic probabilities'}

# Find most common terms for each topic.

perf_top_terms <-  perf_topics %>% 
  group_by(topic) %>% 
  top_n(5, beta) %>% 
  ungroup() %>% 
  arrange(topic, -beta)

perf_top_terms %>% 
  group_by(topic, term) %>%                  
  arrange(desc(beta)) %>%                
  ungroup() %>%
  mutate(term = factor(paste(term, topic, sep = "__"), levels = rev(paste(term, topic, sep = "__")))) %>%
  mutate(term = reorder(term, beta)) %>% 
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip() +
  scale_x_discrete(labels = function(x) gsub("__.+$", "", x)) +
  xlab(NULL)
```

As can be seen in Figure 11, some topics overlap. For example, topics three, five, and seven contain at least one of the terms 'life' and 'people' in the top two terms, whereas topics four, seven, and nine contain the terms 'kids' or 'parents'. It can be observed that there are meaningful differences between the nine topics, with some of these topics in keeping with the perfectionism themes highlighted so far. For example topic two is unique in its use of terms such as 'college' and 'students' and reflects the academic research of Curran and Hill (2017) highlighted previously. Other topics are also unique, such as topics four and seven, but their relevance to perfectionism is not as clear. For example, topic one contains key terms such as 'code', 'music', and 'time', topics not frequently employed in the academic literature. That is not to say that all discussions of perfectionism in the public domain should exactly mirror those in the academic domain, rather, it is interesting to note the diversity of topics discussed outside of the academic setting, as exemplified in this topic model.